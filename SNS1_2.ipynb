{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement 1: Natural Language Processing (NLP)**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mgaWXyReCiW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7SmkW1hqseM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_tokenize(text):\n",
        "    '''\n",
        "    Processes a string of text through various natural language processing steps.\n",
        "\n",
        "    This function performs the following operations:\n",
        "    1. Converts the text to lowercase\n",
        "    2. Removes punctuation\n",
        "    3. Removes stop words\n",
        "    4. Applies lemmatization\n",
        "    5. Tokenizes the text into sentences and words\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text to be processed\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two elements:\n",
        "        - list of str: Processed and tokenized sentences\n",
        "        - list of list of str: Tokenized words for each sentence\n",
        "    '''\n",
        "    # Remove newline characters from the text to ensure continuous text flow without line breaks\n",
        "    cleaned_text = re.sub(r'\\n', '', text)\n",
        "\n",
        "    # Convert the cleaned text to lowercase to ensure uniformity\n",
        "    lower_cleaned_text = cleaned_text.lower()\n",
        "\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(lower_cleaned_text)\n",
        "\n",
        "    # Define the set of stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Initialize the WordNet lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Initialize lists to store processed sentences and words\n",
        "    processed_sentences = []\n",
        "    list_of_words = []\n",
        "\n",
        "    # Process each sentence individually\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence into words\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Remove punctuation from the list of words\n",
        "        words = [word for word in words if word not in string.punctuation]\n",
        "\n",
        "        # Remove stop words and apply lemmatization to the remaining words\n",
        "        filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "        list_of_words.append(filtered_words)\n",
        "\n",
        "        # Join the filtered words back into a single string\n",
        "        processed_sentence = ' '.join(filtered_words)\n",
        "        processed_sentences.append(processed_sentence)\n",
        "\n",
        "    # Return the list of processed sentences and the list of tokenized words\n",
        "    return processed_sentences, list_of_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Problem Statement 2: Text Generation**"
      ],
      "metadata": {
        "id": "gz9zNik2EkHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have decided to use microsoft phi for text generation. Since colab offers limited compute units, I have quantized and double quantized the model with bits and bytes library offered by Hugging face and loaded it in 4bit.\n",
        "Further, I have fed the model a one shot prompt on what type of answer is expected.\n",
        "\n",
        "**Please change the runtime to T4 GPU as quantization requires GPU**"
      ],
      "metadata": {
        "id": "_SmncTIzUhhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install -U bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "b52sozU2-74w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=nf4_config, device_map=\"auto\", trust_remote_code=True, )"
      ],
      "metadata": {
        "id": "NZuEc4cEG2QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in south indian food. When asked about any dish, you give out the ingredients as well as the recipe\"},\n",
        "    {\"role\": \"user\", \"content\": \"How is karam dosa made?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Karam dosa is a spicy version of the traditional South Indian dosa. The term karam means spicy in Telugu, and this dish is particularly popular in the Andhra Pradesh region. Here's a high-level summary of how it's made: Ingredients: Dosa batter: Made from rice and urad dal (black gram). Karam (spicy) chutney: Made from ingredients like red chilies, garlic, onions, and tamarind. Oil or ghee: For cooking the dosa. Toppings: Some variations may include finely chopped onions, curry leaves, and coriander. Method: Prepare the batter: The dosa batter is prepared by soaking rice and urad dal, grinding them to a fine paste, and fermenting the mixture overnight. Make the karam chutney: Blend red chilies, garlic, onions, tamarind, and other spices to create a thick, spicy chutney. Cook the dosa: Heat a tawa (griddle), pour a ladleful of batter, and spread it evenly in a circular motion to form a thin dosa. Apply karam chutney: Once the dosa is partially cooked, spread a layer of the spicy chutney on top. Cook until crispy: Add a bit of oil or ghee around the edges and cook until the dosa is crispy and golden brown. Serve: Fold the dosa and serve hot, often with coconut chutney and samba\"},\n",
        "    {\"role\": \"user\", \"content\": \"How is rava dosa made?\"},\n",
        "]\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "c3Ihkc1sIgC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}